{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intrusion_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "26_pQ-K8IaM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install --upgrade \"tensorflow-gpu==2.2\" \"keras>=2.0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH5DejQk-Vfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = '/content/drive/My Drive/Colab Notebooks/Intrusion/nslkdd_train_mod.csv'\n",
        "\n",
        "df = pd.read_csv(path)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4O-QxV4GE6_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "db107411-53dc-47d1-b545-8e6a65a23c1e"
      },
      "source": [
        "df[0:5]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>duration</th>\n",
              "      <th>protocol_type</th>\n",
              "      <th>service</th>\n",
              "      <th>flag</th>\n",
              "      <th>src_bytes</th>\n",
              "      <th>dst_bytes</th>\n",
              "      <th>land</th>\n",
              "      <th>wrong_fragment</th>\n",
              "      <th>urgent</th>\n",
              "      <th>hot</th>\n",
              "      <th>num_failed_logins</th>\n",
              "      <th>logged_in</th>\n",
              "      <th>num_compromised</th>\n",
              "      <th>root_shell</th>\n",
              "      <th>su_attempted</th>\n",
              "      <th>num_root</th>\n",
              "      <th>num_file_creations</th>\n",
              "      <th>num_shells</th>\n",
              "      <th>num_access_files</th>\n",
              "      <th>num_outbound_cmds</th>\n",
              "      <th>is_host_login</th>\n",
              "      <th>is_guest_login</th>\n",
              "      <th>count</th>\n",
              "      <th>srv_count</th>\n",
              "      <th>serror_rate</th>\n",
              "      <th>srv_serror_rate</th>\n",
              "      <th>rerror_rate</th>\n",
              "      <th>srv_rerror_rate</th>\n",
              "      <th>same_srv_rate</th>\n",
              "      <th>diff_srv_rate</th>\n",
              "      <th>srv_diff_host_rate</th>\n",
              "      <th>dst_host_count</th>\n",
              "      <th>dst_host_srv_count</th>\n",
              "      <th>dst_host_same_srv_rate</th>\n",
              "      <th>dst_host_diff_srv_rate</th>\n",
              "      <th>dst_host_same_src_port_rate</th>\n",
              "      <th>dst_host_srv_diff_host_rate</th>\n",
              "      <th>dst_host_serror_rate</th>\n",
              "      <th>dst_host_srv_serror_rate</th>\n",
              "      <th>dst_host_rerror_rate</th>\n",
              "      <th>dst_host_srv_rerror_rate</th>\n",
              "      <th>outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>ftp_data</td>\n",
              "      <td>SF</td>\n",
              "      <td>491</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>150</td>\n",
              "      <td>25</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>udp</td>\n",
              "      <td>other</td>\n",
              "      <td>SF</td>\n",
              "      <td>146</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>255</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>private</td>\n",
              "      <td>S0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>123</td>\n",
              "      <td>6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>255</td>\n",
              "      <td>26</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>DOS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>232</td>\n",
              "      <td>8153</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>30</td>\n",
              "      <td>255</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>199</td>\n",
              "      <td>420</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.09</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   duration protocol_type  ... dst_host_srv_rerror_rate outcome\n",
              "0         0           tcp  ...                     0.00  Normal\n",
              "1         0           udp  ...                     0.00  Normal\n",
              "2         0           tcp  ...                     0.00     DOS\n",
              "3         0           tcp  ...                     0.01  Normal\n",
              "4         0           tcp  ...                     0.00  Normal\n",
              "\n",
              "[5 rows x 42 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmDwjUosG0a5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from scipy.stats import zscore\n",
        "\n",
        "def expand_categories(values):\n",
        "    result = []\n",
        "    s = values.value_counts()\n",
        "    t = float(len(values))\n",
        "    for v in s.index:\n",
        "        result.append(\"{}:{}%\".format(v,round(100*(s[v]/t),2)))\n",
        "    return \"[{}]\".format(\",\".join(result))\n",
        "        \n",
        "def analyze(df):\n",
        "    print()\n",
        "    cols = df.columns.values\n",
        "    total = float(len(df))\n",
        "\n",
        "    print(\"{} rows\".format(int(total)))\n",
        "    for col in cols:\n",
        "        uniques = df[col].unique()\n",
        "        unique_count = len(uniques)\n",
        "        if unique_count>100:\n",
        "            print(\"** {}:{} ({}%)\".format(col,unique_count,int(((unique_count)/total)*100)))\n",
        "        else:\n",
        "            print(\"** {}:{}\".format(col,expand_categories(df[col])))\n",
        "            expand_categories(df[col])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKHj63uDX6En",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "outputId": "3731bcaa-9339-4164-bd80-0486a8c6059b"
      },
      "source": [
        "analyze(df)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "125973 rows\n",
            "** duration:2981 (2%)\n",
            "** protocol_type:[tcp:81.52%,udp:11.9%,icmp:6.58%]\n",
            "** service:[http:32.02%,private:17.35%,domain_u:7.18%,smtp:5.81%,ftp_data:5.45%,eco_i:3.64%,other:3.46%,ecr_i:2.44%,telnet:1.87%,finger:1.4%,ftp:1.39%,auth:0.76%,Z39_50:0.68%,uucp:0.62%,courier:0.58%,bgp:0.56%,whois:0.55%,uucp_path:0.55%,iso_tsap:0.55%,time:0.52%,R2L4:0.51%,nnsp:0.5%,vmnet:0.49%,urp_i:0.48%,domain:0.45%,ctf:0.45%,csnet_ns:0.43%,supdup:0.43%,discard:0.43%,http_443:0.42%,daytime:0.41%,gopher:0.41%,efs:0.39%,systat:0.38%,link:0.38%,exec:0.38%,hostnames:0.37%,name:0.36%,mtp:0.35%,echo:0.34%,klogin:0.34%,login:0.34%,ldap:0.33%,netbios_dgm:0.32%,sunrpc:0.3%,netbios_ssn:0.29%,netstat:0.29%,netbios_ns:0.28%,ssh:0.25%,kshell:0.24%,nntp:0.23%,pop_3:0.21%,sql_net:0.19%,IRC:0.15%,ntp_u:0.13%,rje:0.07%,remote_job:0.06%,pop_2:0.06%,X11:0.06%,printer:0.05%,shell:0.05%,urh_i:0.01%,red_i:0.01%,tim_i:0.01%,pm_dump:0.0%,tftp_u:0.0%,aol:0.0%,http_8001:0.0%,harvest:0.0%,http_2784:0.0%]\n",
            "** flag:[SF:59.49%,S0:27.67%,REJ:8.92%,RSTR:1.92%,RSTO:1.24%,S1:0.29%,SH:0.22%,S2:0.1%,RSTOS0:0.08%,S3:0.04%,OTH:0.04%]\n",
            "** src_bytes:3341 (2%)\n",
            "** dst_bytes:9326 (7%)\n",
            "** land:[0:99.98%,1:0.02%]\n",
            "** wrong_fragment:[0:99.13%,3:0.7%,1:0.16%]\n",
            "** urgent:[0:99.99%,1:0.0%,2:0.0%,3:0.0%]\n",
            "** hot:[0:97.88%,2:0.82%,1:0.29%,28:0.22%,30:0.2%,4:0.14%,6:0.11%,5:0.06%,24:0.05%,19:0.05%,22:0.04%,3:0.04%,18:0.04%,14:0.02%,20:0.01%,7:0.0%,15:0.0%,11:0.0%,44:0.0%,9:0.0%,25:0.0%,8:0.0%,10:0.0%,12:0.0%,33:0.0%,17:0.0%,21:0.0%,77:0.0%]\n",
            "** num_failed_logins:[0:99.9%,1:0.08%,2:0.01%,3:0.0%,4:0.0%,5:0.0%]\n",
            "** logged_in:[0:60.43%,1:39.57%]\n",
            "** num_compromised:[0:98.98%,1:0.77%,2:0.08%,4:0.03%,3:0.03%,6:0.02%,5:0.01%,7:0.0%,9:0.0%,8:0.0%,21:0.0%,31:0.0%,13:0.0%,12:0.0%,151:0.0%,10:0.0%,23:0.0%,371:0.0%,884:0.0%,520:0.0%,15:0.0%,22:0.0%,789:0.0%,405:0.0%,193:0.0%,1043:0.0%,83:0.0%,19:0.0%,258:0.0%,18:0.0%,17:0.0%,16:0.0%,462:0.0%,456:0.0%,78:0.0%,14:0.0%,452:0.0%,716:0.0%,1739:0.0%,75:0.0%,11:0.0%,198:0.0%,74:0.0%,457:0.0%,202:0.0%,767:0.0%,407:0.0%,756:0.0%,174:0.0%,558:0.0%,622:0.0%,175:0.0%,751:0.0%,177:0.0%,307:0.0%,691:0.0%,7479:0.0%,187:0.0%,373:0.0%,54:0.0%,247:0.0%,375:0.0%,568:0.0%,121:0.0%,761:0.0%,378:0.0%,110:0.0%,46:0.0%,237:0.0%,44:0.0%,281:0.0%,345:0.0%,537:0.0%,538:0.0%,157:0.0%,349:0.0%,94:0.0%,543:0.0%,676:0.0%,37:0.0%,38:0.0%,102:0.0%,166:0.0%,40:0.0%,809:0.0%,682:0.0%,107:0.0%,217:0.0%]\n",
            "** root_shell:[0:99.87%,1:0.13%]\n",
            "** su_attempted:[0:99.94%,2:0.05%,1:0.02%]\n",
            "** num_root:[0:99.48%,1:0.22%,9:0.1%,6:0.08%,2:0.03%,5:0.02%,4:0.01%,3:0.01%,7:0.0%,36:0.0%,421:0.0%,39:0.0%,10:0.0%,857:0.0%,71:0.0%,8:0.0%,151:0.0%,278:0.0%,22:0.0%,1045:0.0%,789:0.0%,402:0.0%,338:0.0%,146:0.0%,512:0.0%,849:0.0%,450:0.0%,17:0.0%,387:0.0%,16:0.0%,1743:0.0%,975:0.0%,206:0.0%,14:0.0%,77:0.0%,218:0.0%,12:0.0%,74:0.0%,261:0.0%,841:0.0%,390:0.0%,204:0.0%,191:0.0%,91:0.0%,121:0.0%,629:0.0%,54:0.0%,502:0.0%,55:0.0%,119:0.0%,247:0.0%,184:0.0%,505:0.0%,754:0.0%,889:0.0%,123:0.0%,187:0.0%,508:0.0%,572:0.0%,190:0.0%,446:0.0%,179:0.0%,626:0.0%,766:0.0%,867:0.0%,222:0.0%,287:0.0%,416:0.0%,417:0.0%,993:0.0%,610:0.0%,611:0.0%,100:0.0%,7468:0.0%,38:0.0%,40:0.0%,104:0.0%,425:0.0%,684:0.0%,749:0.0%,47:0.0%,605:0.0%]\n",
            "** num_file_creations:[0:99.77%,1:0.12%,2:0.03%,4:0.01%,15:0.0%,3:0.0%,5:0.0%,8:0.0%,10:0.0%,17:0.0%,12:0.0%,7:0.0%,11:0.0%,18:0.0%,26:0.0%,6:0.0%,14:0.0%,25:0.0%,20:0.0%,40:0.0%,13:0.0%,23:0.0%,9:0.0%,16:0.0%,29:0.0%,38:0.0%,36:0.0%,34:0.0%,33:0.0%,28:0.0%,22:0.0%,21:0.0%,27:0.0%,19:0.0%,43:0.0%]\n",
            "** num_shells:[0:99.96%,1:0.03%,2:0.0%]\n",
            "** num_access_files:[0:99.71%,1:0.25%,2:0.02%,3:0.01%,5:0.0%,4:0.0%,6:0.0%,8:0.0%,7:0.0%,9:0.0%]\n",
            "** num_outbound_cmds:[0:100.0%]\n",
            "** is_host_login:[0:100.0%,1:0.0%]\n",
            "** is_guest_login:[0:99.06%,1:0.94%]\n",
            "** count:512 (0%)\n",
            "** srv_count:509 (0%)\n",
            "** serror_rate:[0.0:68.93%,1.0:27.34%,0.5:0.39%,0.33:0.25%,0.07:0.24%,0.06:0.24%,0.08:0.2%,0.99:0.2%,0.01:0.17%,0.25:0.17%,0.05:0.15%,0.09:0.15%,0.1:0.14%,0.2:0.12%,0.03:0.12%,0.04:0.1%,0.17:0.09%,0.11:0.09%,0.14:0.08%,0.02:0.07%,0.97:0.06%,0.12:0.06%,0.98:0.05%,0.16:0.04%,0.96:0.03%,0.13:0.03%,0.21:0.03%,0.23:0.03%,0.18:0.03%,0.22:0.03%,0.95:0.02%,0.15:0.02%,0.94:0.02%,0.19:0.02%,0.93:0.01%,0.64:0.01%,0.75:0.01%,0.67:0.01%,0.83:0.01%,0.92:0.01%,0.8:0.01%,0.79:0.01%,0.65:0.01%,0.78:0.01%,0.63:0.01%,0.86:0.01%,0.24:0.01%,0.85:0.01%,0.66:0.01%,0.68:0.01%,0.74:0.01%,0.62:0.01%,0.91:0.01%,0.69:0.01%,0.59:0.01%,0.7:0.01%,0.58:0.01%,0.81:0.01%,0.6:0.01%,0.29:0.01%,0.9:0.0%,0.88:0.0%,0.89:0.0%,0.72:0.0%,0.73:0.0%,0.4:0.0%,0.76:0.0%,0.71:0.0%,0.77:0.0%,0.27:0.0%,0.61:0.0%,0.52:0.0%,0.84:0.0%,0.51:0.0%,0.53:0.0%,0.44:0.0%,0.56:0.0%,0.54:0.0%,0.55:0.0%,0.82:0.0%,0.43:0.0%,0.26:0.0%,0.28:0.0%,0.57:0.0%,0.35:0.0%,0.36:0.0%,0.37:0.0%,0.87:0.0%,0.3:0.0%]\n",
            "** srv_serror_rate:[0.0:70.45%,1.0:27.68%,0.5:0.34%,0.33:0.22%,0.25:0.18%,0.2:0.1%,0.17:0.09%,0.05:0.07%,0.03:0.07%,0.04:0.07%,0.07:0.05%,0.06:0.05%,0.08:0.05%,0.02:0.05%,0.14:0.05%,0.67:0.04%,0.12:0.04%,0.09:0.03%,0.1:0.03%,0.95:0.03%,0.11:0.03%,0.22:0.02%,0.29:0.02%,0.4:0.01%,0.91:0.01%,0.75:0.01%,0.94:0.01%,0.92:0.01%,0.89:0.01%,0.8:0.01%,0.88:0.01%,0.9:0.01%,0.18:0.01%,0.93:0.01%,0.83:0.01%,0.86:0.01%,0.13:0.01%,0.01:0.0%,0.15:0.0%,0.58:0.0%,0.23:0.0%,0.27:0.0%,0.41:0.0%,0.3:0.0%,0.43:0.0%,0.6:0.0%,0.59:0.0%,0.37:0.0%,0.26:0.0%,0.35:0.0%,0.76:0.0%,0.39:0.0%,0.56:0.0%,0.71:0.0%,0.34:0.0%,0.36:0.0%,0.53:0.0%,0.32:0.0%,0.78:0.0%,0.38:0.0%,0.87:0.0%,0.45:0.0%,0.85:0.0%,0.74:0.0%,0.55:0.0%,0.54:0.0%,0.44:0.0%,0.61:0.0%,0.72:0.0%,0.96:0.0%,0.63:0.0%,0.82:0.0%,0.28:0.0%,0.24:0.0%,0.48:0.0%,0.47:0.0%,0.19:0.0%,0.73:0.0%,0.68:0.0%,0.21:0.0%,0.51:0.0%,0.46:0.0%,0.65:0.0%,0.42:0.0%,0.7:0.0%,0.57:0.0%]\n",
            "** rerror_rate:[0.0:87.15%,1.0:10.22%,0.9:0.21%,0.92:0.17%,0.93:0.17%,0.89:0.16%,0.91:0.15%,0.5:0.13%,0.88:0.11%,0.95:0.11%,0.94:0.1%,0.03:0.08%,0.25:0.07%,0.87:0.07%,0.33:0.07%,0.02:0.06%,0.86:0.06%,0.96:0.06%,0.2:0.05%,0.01:0.05%,0.04:0.04%,0.82:0.04%,0.85:0.04%,0.84:0.03%,0.78:0.03%,0.67:0.03%,0.8:0.03%,0.05:0.03%,0.76:0.03%,0.75:0.03%,0.79:0.03%,0.17:0.03%,0.97:0.03%,0.77:0.03%,0.83:0.02%,0.81:0.02%,0.06:0.02%,0.07:0.02%,0.08:0.02%,0.99:0.02%,0.98:0.01%,0.14:0.01%,0.12:0.01%,0.36:0.01%,0.71:0.01%,0.1:0.01%,0.21:0.01%,0.4:0.01%,0.11:0.01%,0.09:0.01%,0.35:0.01%,0.22:0.01%,0.73:0.01%,0.64:0.01%,0.37:0.01%,0.32:0.0%,0.26:0.0%,0.43:0.0%,0.29:0.0%,0.31:0.0%,0.62:0.0%,0.69:0.0%,0.34:0.0%,0.27:0.0%,0.56:0.0%,0.57:0.0%,0.3:0.0%,0.6:0.0%,0.23:0.0%,0.28:0.0%,0.74:0.0%,0.72:0.0%,0.24:0.0%,0.19:0.0%,0.38:0.0%,0.7:0.0%,0.18:0.0%,0.44:0.0%,0.65:0.0%,0.55:0.0%,0.63:0.0%,0.58:0.0%]\n",
            "** srv_rerror_rate:[0.0:87.14%,1.0:11.77%,0.5:0.19%,0.33:0.13%,0.25:0.09%,0.2:0.07%,0.17:0.06%,0.04:0.04%,0.03:0.04%,0.14:0.04%,0.05:0.03%,0.67:0.03%,0.02:0.03%,0.08:0.03%,0.06:0.03%,0.12:0.03%,0.07:0.02%,0.1:0.02%,0.09:0.02%,0.11:0.02%,0.4:0.02%,0.75:0.01%,0.29:0.01%,0.83:0.01%,0.8:0.01%,0.85:0.01%,0.6:0.01%,0.82:0.01%,0.71:0.01%,0.79:0.01%,0.22:0.0%,0.84:0.0%,0.69:0.0%,0.64:0.0%,0.81:0.0%,0.73:0.0%,0.62:0.0%,0.86:0.0%,0.43:0.0%,0.78:0.0%,0.88:0.0%,0.57:0.0%,0.76:0.0%,0.87:0.0%,0.89:0.0%,0.01:0.0%,0.56:0.0%,0.74:0.0%,0.38:0.0%,0.77:0.0%,0.96:0.0%,0.92:0.0%,0.72:0.0%,0.7:0.0%,0.9:0.0%,0.58:0.0%,0.95:0.0%,0.55:0.0%,0.15:0.0%,0.13:0.0%,0.18:0.0%,0.3:0.0%]\n",
            "** same_srv_rate:101 (0%)\n",
            "** diff_srv_rate:[0.0:60.5%,0.06:15.08%,0.07:7.55%,0.05:5.47%,1.0:2.73%,0.08:1.49%,0.01:0.8%,0.09:0.51%,0.04:0.5%,0.5:0.44%,0.67:0.36%,0.1:0.35%,0.12:0.31%,0.11:0.3%,0.03:0.22%,0.6:0.21%,0.02:0.21%,0.29:0.18%,0.14:0.18%,0.33:0.17%,0.13:0.17%,0.15:0.15%,0.17:0.15%,0.25:0.15%,0.4:0.15%,0.52:0.12%,0.18:0.11%,0.2:0.11%,0.22:0.1%,0.19:0.08%,0.16:0.08%,0.38:0.07%,0.21:0.06%,0.75:0.06%,0.27:0.06%,0.24:0.04%,0.36:0.04%,0.53:0.04%,0.57:0.04%,0.31:0.03%,0.54:0.03%,0.23:0.03%,0.95:0.03%,0.99:0.03%,0.43:0.03%,0.3:0.03%,0.42:0.03%,0.44:0.03%,0.96:0.02%,0.55:0.02%,0.41:0.02%,0.28:0.02%,0.8:0.02%,0.26:0.02%,0.37:0.02%,0.56:0.02%,0.39:0.01%,0.58:0.01%,0.32:0.01%,0.71:0.01%,0.45:0.01%,0.64:0.01%,0.51:0.01%,0.62:0.01%,0.69:0.01%,0.46:0.01%,0.35:0.01%,0.73:0.01%,0.83:0.01%,0.47:0.01%,0.78:0.01%,0.82:0.01%,0.97:0.01%,0.98:0.0%,0.77:0.0%,0.7:0.0%,0.59:0.0%,0.92:0.0%,0.61:0.0%,0.76:0.0%,0.79:0.0%,0.88:0.0%,0.86:0.0%,0.74:0.0%,0.72:0.0%,0.65:0.0%,0.87:0.0%,0.85:0.0%,0.81:0.0%,0.89:0.0%,0.63:0.0%,0.9:0.0%,0.48:0.0%,0.91:0.0%,0.68:0.0%]\n",
            "** srv_diff_host_rate:[0.0:77.46%,1.0:6.46%,0.01:2.27%,0.5:0.78%,0.67:0.77%,0.12:0.72%,0.33:0.63%,0.02:0.61%,0.11:0.58%,0.25:0.57%,0.1:0.57%,0.14:0.53%,0.08:0.52%,0.15:0.49%,0.4:0.49%,0.09:0.49%,0.17:0.49%,0.29:0.47%,0.2:0.46%,0.18:0.46%,0.22:0.42%,0.06:0.41%,0.07:0.41%,0.13:0.37%,0.05:0.26%,0.19:0.2%,0.75:0.19%,0.27:0.18%,0.03:0.17%,0.21:0.17%,0.16:0.15%,0.04:0.15%,0.43:0.14%,0.6:0.14%,0.3:0.13%,0.38:0.13%,0.23:0.13%,0.24:0.07%,0.31:0.06%,0.36:0.06%,0.8:0.05%,0.44:0.04%,0.57:0.03%,0.28:0.02%,0.26:0.01%,0.45:0.01%,0.56:0.01%,0.42:0.01%,0.71:0.01%,0.32:0.01%,0.35:0.01%,0.62:0.01%,0.83:0.01%,0.47:0.0%,0.37:0.0%,0.54:0.0%,0.55:0.0%,0.46:0.0%,0.88:0.0%,0.41:0.0%]\n",
            "** dst_host_count:256 (0%)\n",
            "** dst_host_srv_count:256 (0%)\n",
            "** dst_host_same_srv_rate:101 (0%)\n",
            "** dst_host_diff_srv_rate:101 (0%)\n",
            "** dst_host_same_src_port_rate:101 (0%)\n",
            "** dst_host_srv_diff_host_rate:[0.0:68.99%,0.02:6.31%,0.01:5.67%,0.03:3.75%,0.04:3.59%,0.05:2.42%,0.5:1.23%,0.06:1.06%,0.07:0.82%,0.25:0.75%,1.0:0.55%,0.51:0.49%,0.08:0.39%,0.09:0.33%,0.26:0.26%,0.11:0.22%,0.15:0.2%,0.52:0.19%,0.13:0.19%,0.12:0.19%,0.1:0.19%,0.14:0.19%,0.16:0.18%,0.2:0.17%,0.17:0.17%,0.18:0.16%,0.22:0.14%,0.21:0.12%,0.19:0.11%,0.29:0.1%,0.27:0.09%,0.33:0.08%,0.4:0.08%,0.67:0.08%,0.24:0.06%,0.53:0.06%,0.23:0.05%,0.54:0.04%,0.3:0.03%,0.28:0.03%,0.56:0.03%,0.31:0.03%,0.6:0.03%,0.57:0.02%,0.55:0.02%,0.38:0.02%,0.43:0.02%,0.75:0.01%,0.32:0.01%,0.34:0.01%,0.35:0.01%,0.44:0.01%,0.37:0.01%,0.45:0.01%,0.62:0.0%,0.39:0.0%,0.47:0.0%,0.42:0.0%,0.36:0.0%,0.46:0.0%,0.48:0.0%,0.8:0.0%,0.41:0.0%,0.49:0.0%,0.58:0.0%,0.73:0.0%,0.97:0.0%,0.86:0.0%,0.83:0.0%,0.64:0.0%,0.88:0.0%,0.7:0.0%,0.78:0.0%,0.93:0.0%,0.71:0.0%]\n",
            "** dst_host_serror_rate:101 (0%)\n",
            "** dst_host_srv_serror_rate:[0.0:67.76%,1.0:27.19%,0.01:2.99%,0.02:0.51%,0.03:0.13%,0.04:0.09%,0.5:0.08%,0.05:0.06%,0.08:0.06%,0.07:0.06%,0.09:0.05%,0.97:0.04%,0.98:0.04%,0.06:0.04%,0.67:0.04%,0.11:0.04%,0.12:0.04%,0.33:0.04%,0.96:0.03%,0.1:0.03%,0.17:0.03%,0.25:0.03%,0.14:0.02%,0.2:0.02%,0.95:0.02%,0.92:0.02%,0.94:0.02%,0.75:0.02%,0.93:0.02%,0.91:0.02%,0.78:0.02%,0.88:0.02%,0.9:0.01%,0.16:0.01%,0.62:0.01%,0.73:0.01%,0.79:0.01%,0.27:0.01%,0.6:0.01%,0.18:0.01%,0.56:0.01%,0.65:0.01%,0.13:0.01%,0.43:0.01%,0.22:0.01%,0.4:0.01%,0.89:0.01%,0.85:0.01%,0.29:0.01%,0.15:0.01%,0.83:0.01%,0.69:0.01%,0.86:0.01%,0.8:0.01%,0.48:0.01%,0.87:0.01%,0.7:0.01%,0.81:0.01%,0.36:0.01%,0.76:0.01%,0.34:0.01%,0.49:0.01%,0.31:0.01%,0.71:0.01%,0.66:0.01%,0.82:0.01%,0.57:0.01%,0.3:0.01%,0.51:0.01%,0.39:0.01%,0.64:0.01%,0.38:0.01%,0.26:0.01%,0.63:0.01%,0.21:0.01%,0.41:0.01%,0.47:0.01%,0.19:0.01%,0.24:0.0%,0.52:0.0%,0.74:0.0%,0.54:0.0%,0.58:0.0%,0.84:0.0%,0.77:0.0%,0.45:0.0%,0.68:0.0%,0.42:0.0%,0.61:0.0%,0.28:0.0%,0.32:0.0%,0.55:0.0%,0.72:0.0%,0.44:0.0%,0.35:0.0%,0.53:0.0%,0.37:0.0%,0.23:0.0%,0.59:0.0%,0.46:0.0%]\n",
            "** dst_host_rerror_rate:101 (0%)\n",
            "** dst_host_srv_rerror_rate:101 (0%)\n",
            "** outcome:[Normal:53.46%,DOS:36.46%,Probes:9.25%,R2L:0.79%,U2R:0.04%]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TprHzsQeYEsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
        "    if mean is None:\n",
        "        mean = df[name].mean()\n",
        "\n",
        "    if sd is None:\n",
        "        sd = df[name].std()\n",
        "\n",
        "    df[name] = (df[name] - mean) / sd\n",
        "    \n",
        "def encode_text_dummy(df, name):\n",
        "    dummies = pd.get_dummies(df[name])\n",
        "    for x in dummies.columns:\n",
        "        dummy_name = f\"{name}-{x}\"\n",
        "        df[dummy_name] = dummies[x]\n",
        "    df.drop(name, axis=1, inplace=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_TdbroaYMla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encode_numeric_zscore(df, 'duration')\n",
        "encode_text_dummy(df, 'protocol_type')\n",
        "encode_text_dummy(df, 'service')\n",
        "encode_text_dummy(df, 'flag')\n",
        "encode_numeric_zscore(df, 'src_bytes')\n",
        "encode_numeric_zscore(df, 'dst_bytes')\n",
        "encode_text_dummy(df, 'land')\n",
        "encode_numeric_zscore(df, 'wrong_fragment')\n",
        "encode_numeric_zscore(df, 'urgent')\n",
        "encode_numeric_zscore(df, 'hot')\n",
        "encode_numeric_zscore(df, 'num_failed_logins')\n",
        "encode_text_dummy(df, 'logged_in')\n",
        "encode_numeric_zscore(df, 'num_compromised')\n",
        "encode_numeric_zscore(df, 'root_shell')\n",
        "encode_numeric_zscore(df, 'su_attempted')\n",
        "encode_numeric_zscore(df, 'num_root')\n",
        "encode_numeric_zscore(df, 'num_file_creations')\n",
        "encode_numeric_zscore(df, 'num_shells')\n",
        "encode_numeric_zscore(df, 'num_access_files')\n",
        "encode_numeric_zscore(df, 'num_outbound_cmds')\n",
        "encode_text_dummy(df, 'is_host_login')\n",
        "encode_text_dummy(df, 'is_guest_login')\n",
        "encode_numeric_zscore(df, 'count')\n",
        "encode_numeric_zscore(df, 'srv_count')\n",
        "encode_numeric_zscore(df, 'serror_rate')\n",
        "encode_numeric_zscore(df, 'srv_serror_rate')\n",
        "encode_numeric_zscore(df, 'rerror_rate')\n",
        "encode_numeric_zscore(df, 'srv_rerror_rate')\n",
        "encode_numeric_zscore(df, 'same_srv_rate')\n",
        "encode_numeric_zscore(df, 'diff_srv_rate')\n",
        "encode_numeric_zscore(df, 'srv_diff_host_rate')\n",
        "encode_numeric_zscore(df, 'dst_host_count')\n",
        "encode_numeric_zscore(df, 'dst_host_srv_count')\n",
        "encode_numeric_zscore(df, 'dst_host_same_srv_rate')\n",
        "encode_numeric_zscore(df, 'dst_host_diff_srv_rate')\n",
        "encode_numeric_zscore(df, 'dst_host_same_src_port_rate')\n",
        "encode_numeric_zscore(df, 'dst_host_srv_diff_host_rate')\n",
        "encode_numeric_zscore(df, 'dst_host_serror_rate')\n",
        "encode_numeric_zscore(df, 'dst_host_srv_serror_rate')\n",
        "encode_numeric_zscore(df, 'dst_host_rerror_rate')\n",
        "encode_numeric_zscore(df, 'dst_host_srv_rerror_rate')\n",
        "\n",
        "df.dropna(inplace=True,axis=1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWsxRfXfjNC3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "ae2cfe5e-a31f-4e1b-804a-e3cb8709bdbf"
      },
      "source": [
        "df[0:5]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>duration</th>\n",
              "      <th>src_bytes</th>\n",
              "      <th>dst_bytes</th>\n",
              "      <th>wrong_fragment</th>\n",
              "      <th>urgent</th>\n",
              "      <th>hot</th>\n",
              "      <th>num_failed_logins</th>\n",
              "      <th>num_compromised</th>\n",
              "      <th>root_shell</th>\n",
              "      <th>su_attempted</th>\n",
              "      <th>num_root</th>\n",
              "      <th>num_file_creations</th>\n",
              "      <th>num_shells</th>\n",
              "      <th>num_access_files</th>\n",
              "      <th>count</th>\n",
              "      <th>srv_count</th>\n",
              "      <th>serror_rate</th>\n",
              "      <th>srv_serror_rate</th>\n",
              "      <th>rerror_rate</th>\n",
              "      <th>srv_rerror_rate</th>\n",
              "      <th>same_srv_rate</th>\n",
              "      <th>diff_srv_rate</th>\n",
              "      <th>srv_diff_host_rate</th>\n",
              "      <th>dst_host_count</th>\n",
              "      <th>dst_host_srv_count</th>\n",
              "      <th>dst_host_same_srv_rate</th>\n",
              "      <th>dst_host_diff_srv_rate</th>\n",
              "      <th>dst_host_same_src_port_rate</th>\n",
              "      <th>dst_host_srv_diff_host_rate</th>\n",
              "      <th>dst_host_serror_rate</th>\n",
              "      <th>dst_host_srv_serror_rate</th>\n",
              "      <th>dst_host_rerror_rate</th>\n",
              "      <th>dst_host_srv_rerror_rate</th>\n",
              "      <th>outcome</th>\n",
              "      <th>protocol_type-icmp</th>\n",
              "      <th>protocol_type-tcp</th>\n",
              "      <th>protocol_type-udp</th>\n",
              "      <th>service-IRC</th>\n",
              "      <th>service-R2L4</th>\n",
              "      <th>service-X11</th>\n",
              "      <th>...</th>\n",
              "      <th>service-private</th>\n",
              "      <th>service-red_i</th>\n",
              "      <th>service-remote_job</th>\n",
              "      <th>service-rje</th>\n",
              "      <th>service-shell</th>\n",
              "      <th>service-smtp</th>\n",
              "      <th>service-sql_net</th>\n",
              "      <th>service-ssh</th>\n",
              "      <th>service-sunrpc</th>\n",
              "      <th>service-supdup</th>\n",
              "      <th>service-systat</th>\n",
              "      <th>service-telnet</th>\n",
              "      <th>service-tftp_u</th>\n",
              "      <th>service-tim_i</th>\n",
              "      <th>service-time</th>\n",
              "      <th>service-urh_i</th>\n",
              "      <th>service-urp_i</th>\n",
              "      <th>service-uucp</th>\n",
              "      <th>service-uucp_path</th>\n",
              "      <th>service-vmnet</th>\n",
              "      <th>service-whois</th>\n",
              "      <th>flag-OTH</th>\n",
              "      <th>flag-REJ</th>\n",
              "      <th>flag-RSTO</th>\n",
              "      <th>flag-RSTOS0</th>\n",
              "      <th>flag-RSTR</th>\n",
              "      <th>flag-S0</th>\n",
              "      <th>flag-S1</th>\n",
              "      <th>flag-S2</th>\n",
              "      <th>flag-S3</th>\n",
              "      <th>flag-SF</th>\n",
              "      <th>flag-SH</th>\n",
              "      <th>land-0</th>\n",
              "      <th>land-1</th>\n",
              "      <th>logged_in-0</th>\n",
              "      <th>logged_in-1</th>\n",
              "      <th>is_host_login-0</th>\n",
              "      <th>is_host_login-1</th>\n",
              "      <th>is_guest_login-0</th>\n",
              "      <th>is_guest_login-1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.110249</td>\n",
              "      <td>-0.007679</td>\n",
              "      <td>-0.004919</td>\n",
              "      <td>-0.089486</td>\n",
              "      <td>-0.007736</td>\n",
              "      <td>-0.095075</td>\n",
              "      <td>-0.027023</td>\n",
              "      <td>-0.011664</td>\n",
              "      <td>-0.036652</td>\n",
              "      <td>-0.024436</td>\n",
              "      <td>-0.012385</td>\n",
              "      <td>-0.02618</td>\n",
              "      <td>-0.01861</td>\n",
              "      <td>-0.041221</td>\n",
              "      <td>-0.717043</td>\n",
              "      <td>-0.354341</td>\n",
              "      <td>-0.637207</td>\n",
              "      <td>-0.631927</td>\n",
              "      <td>-0.374361</td>\n",
              "      <td>-0.37443</td>\n",
              "      <td>0.771280</td>\n",
              "      <td>-0.349682</td>\n",
              "      <td>-0.374558</td>\n",
              "      <td>-0.324062</td>\n",
              "      <td>-0.818887</td>\n",
              "      <td>-0.782364</td>\n",
              "      <td>-0.280281</td>\n",
              "      <td>0.069972</td>\n",
              "      <td>-0.289102</td>\n",
              "      <td>-0.639529</td>\n",
              "      <td>-0.624868</td>\n",
              "      <td>-0.224532</td>\n",
              "      <td>-0.376386</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.110249</td>\n",
              "      <td>-0.007737</td>\n",
              "      <td>-0.004919</td>\n",
              "      <td>-0.089486</td>\n",
              "      <td>-0.007736</td>\n",
              "      <td>-0.095075</td>\n",
              "      <td>-0.027023</td>\n",
              "      <td>-0.011664</td>\n",
              "      <td>-0.036652</td>\n",
              "      <td>-0.024436</td>\n",
              "      <td>-0.012385</td>\n",
              "      <td>-0.02618</td>\n",
              "      <td>-0.01861</td>\n",
              "      <td>-0.041221</td>\n",
              "      <td>-0.620980</td>\n",
              "      <td>-0.368109</td>\n",
              "      <td>-0.637207</td>\n",
              "      <td>-0.631927</td>\n",
              "      <td>-0.374361</td>\n",
              "      <td>-0.37443</td>\n",
              "      <td>-1.321423</td>\n",
              "      <td>0.482199</td>\n",
              "      <td>-0.374558</td>\n",
              "      <td>0.734340</td>\n",
              "      <td>-1.035684</td>\n",
              "      <td>-1.161026</td>\n",
              "      <td>2.736841</td>\n",
              "      <td>2.367728</td>\n",
              "      <td>-0.289102</td>\n",
              "      <td>-0.639529</td>\n",
              "      <td>-0.624868</td>\n",
              "      <td>-0.387633</td>\n",
              "      <td>-0.376386</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.110249</td>\n",
              "      <td>-0.007762</td>\n",
              "      <td>-0.004919</td>\n",
              "      <td>-0.089486</td>\n",
              "      <td>-0.007736</td>\n",
              "      <td>-0.095075</td>\n",
              "      <td>-0.027023</td>\n",
              "      <td>-0.011664</td>\n",
              "      <td>-0.036652</td>\n",
              "      <td>-0.024436</td>\n",
              "      <td>-0.012385</td>\n",
              "      <td>-0.02618</td>\n",
              "      <td>-0.01861</td>\n",
              "      <td>-0.041221</td>\n",
              "      <td>0.339646</td>\n",
              "      <td>-0.299272</td>\n",
              "      <td>1.602658</td>\n",
              "      <td>1.605097</td>\n",
              "      <td>-0.374361</td>\n",
              "      <td>-0.37443</td>\n",
              "      <td>-1.389663</td>\n",
              "      <td>0.038529</td>\n",
              "      <td>-0.374558</td>\n",
              "      <td>0.734340</td>\n",
              "      <td>-0.809854</td>\n",
              "      <td>-0.938283</td>\n",
              "      <td>-0.174417</td>\n",
              "      <td>-0.480195</td>\n",
              "      <td>-0.289102</td>\n",
              "      <td>1.608753</td>\n",
              "      <td>1.618949</td>\n",
              "      <td>-0.387633</td>\n",
              "      <td>-0.376386</td>\n",
              "      <td>DOS</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.110249</td>\n",
              "      <td>-0.007723</td>\n",
              "      <td>-0.002891</td>\n",
              "      <td>-0.089486</td>\n",
              "      <td>-0.007736</td>\n",
              "      <td>-0.095075</td>\n",
              "      <td>-0.027023</td>\n",
              "      <td>-0.011664</td>\n",
              "      <td>-0.036652</td>\n",
              "      <td>-0.024436</td>\n",
              "      <td>-0.012385</td>\n",
              "      <td>-0.02618</td>\n",
              "      <td>-0.01861</td>\n",
              "      <td>-0.041221</td>\n",
              "      <td>-0.690844</td>\n",
              "      <td>-0.313040</td>\n",
              "      <td>-0.189234</td>\n",
              "      <td>-0.184522</td>\n",
              "      <td>-0.374361</td>\n",
              "      <td>-0.37443</td>\n",
              "      <td>0.771280</td>\n",
              "      <td>-0.349682</td>\n",
              "      <td>-0.374558</td>\n",
              "      <td>-1.533663</td>\n",
              "      <td>1.258749</td>\n",
              "      <td>1.066397</td>\n",
              "      <td>-0.439076</td>\n",
              "      <td>-0.383107</td>\n",
              "      <td>0.066252</td>\n",
              "      <td>-0.572081</td>\n",
              "      <td>-0.602430</td>\n",
              "      <td>-0.387633</td>\n",
              "      <td>-0.345083</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.110249</td>\n",
              "      <td>-0.007728</td>\n",
              "      <td>-0.004814</td>\n",
              "      <td>-0.089486</td>\n",
              "      <td>-0.007736</td>\n",
              "      <td>-0.095075</td>\n",
              "      <td>-0.027023</td>\n",
              "      <td>-0.011664</td>\n",
              "      <td>-0.036652</td>\n",
              "      <td>-0.024436</td>\n",
              "      <td>-0.012385</td>\n",
              "      <td>-0.02618</td>\n",
              "      <td>-0.01861</td>\n",
              "      <td>-0.041221</td>\n",
              "      <td>-0.472520</td>\n",
              "      <td>0.058678</td>\n",
              "      <td>-0.637207</td>\n",
              "      <td>-0.631927</td>\n",
              "      <td>-0.374361</td>\n",
              "      <td>-0.37443</td>\n",
              "      <td>0.771280</td>\n",
              "      <td>-0.349682</td>\n",
              "      <td>-0.028179</td>\n",
              "      <td>0.734340</td>\n",
              "      <td>1.258749</td>\n",
              "      <td>1.066397</td>\n",
              "      <td>-0.439076</td>\n",
              "      <td>-0.480195</td>\n",
              "      <td>-0.289102</td>\n",
              "      <td>-0.639529</td>\n",
              "      <td>-0.624868</td>\n",
              "      <td>-0.387633</td>\n",
              "      <td>-0.376386</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 126 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   duration  src_bytes  ...  is_guest_login-0  is_guest_login-1\n",
              "0 -0.110249  -0.007679  ...                 1                 0\n",
              "1 -0.110249  -0.007737  ...                 1                 0\n",
              "2 -0.110249  -0.007762  ...                 1                 0\n",
              "3 -0.110249  -0.007723  ...                 1                 0\n",
              "4 -0.110249  -0.007728  ...                 1                 0\n",
              "\n",
              "[5 rows x 126 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gdUGDCcYS4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_columns = df.columns.drop('outcome')\n",
        "x = df[x_columns].values\n",
        "dummies = pd.get_dummies(df['outcome'])\n",
        "outcomes = dummies.columns\n",
        "num_classes = len(outcomes)\n",
        "y = dummies.values"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU6roc7LYaRV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9cb47660-e369-492a-c10d-02066319bd47"
      },
      "source": [
        "df.groupby('outcome')['outcome'].count()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "outcome\n",
              "DOS       45927\n",
              "Normal    67343\n",
              "Probes    11656\n",
              "R2L         995\n",
              "U2R          52\n",
              "Name: outcome, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yJOCDhqZquw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "af62b151-7c66-4c49-afbc-57e66dd67a4c"
      },
      "source": [
        "print(df.shape)\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(125973, 126)\n",
            "(125973, 125)\n",
            "(125973, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl2sXXJlaKHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKG-F1-DaLob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8bd92c94-c217-4200-95f8-f7a4e159bbb1"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(94479, 125)\n",
            "(31494, 125)\n",
            "(94479, 5)\n",
            "(31494, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNTWEl1SBbp-",
        "colab_type": "text"
      },
      "source": [
        "Dense Layer Simple NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGFH5U4LYfcs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "8774571f-196b-4cab-89d2-01fc0d2cda2c"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(10, input_shape=(x.shape[1],), activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, kernel_initializer='normal'))\n",
        "model.add(Dense(y.shape[1],activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')\n",
        "history = model.fit(x_train,y_train,validation_data=(x_test,y_test), callbacks=[monitor],verbose=2,epochs=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "2953/2953 - 16s - loss: 0.3045 - accuracy: 0.9146 - val_loss: 0.1168 - val_accuracy: 0.9743\n",
            "Epoch 2/1000\n",
            "2953/2953 - 13s - loss: 0.0900 - accuracy: 0.9782 - val_loss: 0.0857 - val_accuracy: 0.9770\n",
            "Epoch 3/1000\n",
            "2953/2953 - 12s - loss: 0.0663 - accuracy: 0.9803 - val_loss: 0.0606 - val_accuracy: 0.9829\n",
            "Epoch 4/1000\n",
            "2953/2953 - 12s - loss: 0.0494 - accuracy: 0.9847 - val_loss: 0.0547 - val_accuracy: 0.9841\n",
            "Epoch 5/1000\n",
            "2953/2953 - 12s - loss: 0.0409 - accuracy: 0.9872 - val_loss: 0.0490 - val_accuracy: 0.9853\n",
            "Epoch 6/1000\n",
            "2953/2953 - 12s - loss: 0.0375 - accuracy: 0.9883 - val_loss: 0.0425 - val_accuracy: 0.9883\n",
            "Epoch 7/1000\n",
            "2953/2953 - 12s - loss: 0.0353 - accuracy: 0.9897 - val_loss: 0.0407 - val_accuracy: 0.9900\n",
            "Epoch 8/1000\n",
            "2953/2953 - 12s - loss: 0.0327 - accuracy: 0.9913 - val_loss: 0.0430 - val_accuracy: 0.9884\n",
            "Epoch 9/1000\n",
            "2953/2953 - 13s - loss: 0.0319 - accuracy: 0.9915 - val_loss: 0.0418 - val_accuracy: 0.9883\n",
            "Epoch 10/1000\n",
            "2953/2953 - 13s - loss: 0.0310 - accuracy: 0.9920 - val_loss: 0.0396 - val_accuracy: 0.9914\n",
            "Epoch 11/1000\n",
            "2953/2953 - 12s - loss: 0.0303 - accuracy: 0.9920 - val_loss: 0.0392 - val_accuracy: 0.9926\n",
            "Epoch 12/1000\n",
            "2953/2953 - 12s - loss: 0.0288 - accuracy: 0.9921 - val_loss: 0.0426 - val_accuracy: 0.9916\n",
            "Epoch 13/1000\n",
            "2953/2953 - 14s - loss: 0.0290 - accuracy: 0.9925 - val_loss: 0.0399 - val_accuracy: 0.9911\n",
            "Epoch 14/1000\n",
            "2953/2953 - 12s - loss: 0.0285 - accuracy: 0.9924 - val_loss: 0.0405 - val_accuracy: 0.9922\n",
            "Epoch 15/1000\n",
            "2953/2953 - 12s - loss: 0.0275 - accuracy: 0.9928 - val_loss: 0.0360 - val_accuracy: 0.9917\n",
            "Epoch 16/1000\n",
            "2953/2953 - 12s - loss: 0.0276 - accuracy: 0.9929 - val_loss: 0.0383 - val_accuracy: 0.9928\n",
            "Epoch 17/1000\n",
            "2953/2953 - 12s - loss: 0.0272 - accuracy: 0.9929 - val_loss: 0.0398 - val_accuracy: 0.9907\n",
            "Epoch 18/1000\n",
            "2953/2953 - 12s - loss: 0.0265 - accuracy: 0.9930 - val_loss: 0.0367 - val_accuracy: 0.9922\n",
            "Epoch 19/1000\n",
            "2953/2953 - 14s - loss: 0.0265 - accuracy: 0.9930 - val_loss: 0.0359 - val_accuracy: 0.9904\n",
            "Epoch 20/1000\n",
            "2953/2953 - 16s - loss: 0.0256 - accuracy: 0.9931 - val_loss: 0.0378 - val_accuracy: 0.9923\n",
            "Epoch 00020: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThJ8TM7yZI0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1695412f-7e6a-4cec-838f-26d65aed91d1"
      },
      "source": [
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "y_eval = np.argmax(y_test,axis=1)\n",
        "score = metrics.accuracy_score(y_eval, pred)\n",
        "print(\"Validation score: {}\".format(score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation score: 0.9922524925382613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTxO3-cLZArv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "57988e7e-38e7-4083-ed03-39a9f1ba12bc"
      },
      "source": [
        "# plot the training loss and accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N = 20\n",
        "H = history\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 10)                1260      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 50)                550       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                510       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 11        \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 10        \n",
            "=================================================================\n",
            "Total params: 2,341\n",
            "Trainable params: 2,341\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiszzHboVuxa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "8f277b48-882a-4a1a-dec5-b46d1eab4447"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# make predictions on the testing set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdxs = model.predict(x_test)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(y_test.argmax(axis=1), predIdxs, target_names=['DOS','Normal','Probes','R2L','U2R' ]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         DOS       1.00      1.00      1.00     11484\n",
            "      Normal       0.99      0.99      0.99     16774\n",
            "      Probes       0.99      0.98      0.98      2947\n",
            "         R2L       0.70      0.83      0.76       274\n",
            "         U2R       0.00      0.00      0.00        15\n",
            "\n",
            "    accuracy                           0.99     31494\n",
            "   macro avg       0.74      0.76      0.75     31494\n",
            "weighted avg       0.99      0.99      0.99     31494\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_t_assOY_6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIyHo_ErBiBa",
        "colab_type": "text"
      },
      "source": [
        "CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECkd9SFxC_4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os,sys\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Lambda\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
        "from keras.datasets import imdb\n",
        "from keras import backend as K\n",
        "#from sklearn.cross_validation import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D,LSTM, GRU, SimpleRNN\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import h5py\n",
        "from keras import callbacks\n",
        "from keras.callbacks import CSVLogger\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9g1X7LXDl8H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2072d2cc-0ba6-44e9-d84e-801c6658a3ac"
      },
      "source": [
        "X_trainc = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n",
        "X_testc = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))\n",
        "print(X_trainc.shape)\n",
        "print(X_testc.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(94479, 125, 1)\n",
            "(31494, 125, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntHV0MreBPD-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3c361790-e94d-4958-9465-fe6526dd5908"
      },
      "source": [
        "lstm_output_size = 70\n",
        "\n",
        "cnn = Sequential()\n",
        "cnn.add(Convolution1D(64, 3, padding=\"same\", activation=\"relu\", input_shape=(125,1)))\n",
        "cnn.add(MaxPooling1D(pool_size=2))\n",
        "cnn.add(LSTM(lstm_output_size))\n",
        "cnn.add(Dropout(0.1))\n",
        "cnn.add(Dense(y.shape[1],activation='softmax'))\n",
        "cnn.compile(loss='categorical_crossentropy', optimizer=\"adam\",metrics=['accuracy'])\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "# checkpointer = callbacks.ModelCheckpoint(filepath=\"results/cnn1results/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='val_acc',mode='max')\n",
        "# csv_logger = CSVLogger('results/cnn1results/cnntrainanalysis1.csv',separator=',', append=False)\n",
        "history2 = cnn.fit(X_trainc, y_train, epochs=50,validation_data=(X_testc, y_test),callbacks=[monitor])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "2953/2953 [==============================] - 56s 19ms/step - loss: 0.1809 - accuracy: 0.9391 - val_loss: 0.0715 - val_accuracy: 0.9748\n",
            "Epoch 2/50\n",
            "2953/2953 [==============================] - 61s 21ms/step - loss: 0.0706 - accuracy: 0.9769 - val_loss: 0.0539 - val_accuracy: 0.9823\n",
            "Epoch 3/50\n",
            "2953/2953 [==============================] - 60s 20ms/step - loss: 0.0570 - accuracy: 0.9824 - val_loss: 0.0509 - val_accuracy: 0.9819\n",
            "Epoch 4/50\n",
            "2953/2953 [==============================] - 56s 19ms/step - loss: 0.0456 - accuracy: 0.9862 - val_loss: 0.0532 - val_accuracy: 0.9843\n",
            "Epoch 5/50\n",
            "2953/2953 [==============================] - 57s 19ms/step - loss: 0.0407 - accuracy: 0.9874 - val_loss: 0.0373 - val_accuracy: 0.9888\n",
            "Epoch 6/50\n",
            "2953/2953 [==============================] - 57s 19ms/step - loss: 0.0385 - accuracy: 0.9880 - val_loss: 0.0368 - val_accuracy: 0.9889\n",
            "Epoch 7/50\n",
            "2953/2953 [==============================] - 56s 19ms/step - loss: 0.0357 - accuracy: 0.9887 - val_loss: 0.0326 - val_accuracy: 0.9895\n",
            "Epoch 8/50\n",
            "2953/2953 [==============================] - 58s 19ms/step - loss: 0.0358 - accuracy: 0.9889 - val_loss: 0.0449 - val_accuracy: 0.9848\n",
            "Epoch 9/50\n",
            "2953/2953 [==============================] - 55s 19ms/step - loss: 0.0303 - accuracy: 0.9905 - val_loss: 0.0330 - val_accuracy: 0.9903\n",
            "Epoch 10/50\n",
            "2953/2953 [==============================] - 57s 19ms/step - loss: 0.0279 - accuracy: 0.9914 - val_loss: 0.0309 - val_accuracy: 0.9910\n",
            "Epoch 11/50\n",
            "2953/2953 [==============================] - 59s 20ms/step - loss: 0.0283 - accuracy: 0.9914 - val_loss: 0.0267 - val_accuracy: 0.9919\n",
            "Epoch 12/50\n",
            "2953/2953 [==============================] - 60s 20ms/step - loss: 0.0249 - accuracy: 0.9923 - val_loss: 0.0294 - val_accuracy: 0.9917\n",
            "Epoch 13/50\n",
            "2953/2953 [==============================] - 56s 19ms/step - loss: 0.0232 - accuracy: 0.9929 - val_loss: 0.0319 - val_accuracy: 0.9911\n",
            "Epoch 14/50\n",
            "2953/2953 [==============================] - 59s 20ms/step - loss: 0.0231 - accuracy: 0.9929 - val_loss: 0.0248 - val_accuracy: 0.9927\n",
            "Epoch 15/50\n",
            "2953/2953 [==============================] - 55s 19ms/step - loss: 0.0231 - accuracy: 0.9927 - val_loss: 0.0293 - val_accuracy: 0.9914\n",
            "Epoch 16/50\n",
            "2953/2953 [==============================] - 58s 20ms/step - loss: 0.0227 - accuracy: 0.9929 - val_loss: 0.0263 - val_accuracy: 0.9924\n",
            "Epoch 17/50\n",
            "2953/2953 [==============================] - 56s 19ms/step - loss: 0.0207 - accuracy: 0.9936 - val_loss: 0.0224 - val_accuracy: 0.9930\n",
            "Epoch 18/50\n",
            "2953/2953 [==============================] - 56s 19ms/step - loss: 0.0203 - accuracy: 0.9934 - val_loss: 0.0234 - val_accuracy: 0.9930\n",
            "Epoch 19/50\n",
            "2953/2953 [==============================] - 59s 20ms/step - loss: 0.0190 - accuracy: 0.9936 - val_loss: 0.0215 - val_accuracy: 0.9936\n",
            "Epoch 20/50\n",
            "2953/2953 [==============================] - 61s 21ms/step - loss: 0.0185 - accuracy: 0.9943 - val_loss: 0.0257 - val_accuracy: 0.9912\n",
            "Epoch 21/50\n",
            "2953/2953 [==============================] - 55s 19ms/step - loss: 0.0185 - accuracy: 0.9942 - val_loss: 0.0238 - val_accuracy: 0.9935\n",
            "Epoch 22/50\n",
            "2953/2953 [==============================] - 58s 20ms/step - loss: 0.0185 - accuracy: 0.9943 - val_loss: 0.0207 - val_accuracy: 0.9935\n",
            "Epoch 23/50\n",
            "2953/2953 [==============================] - 56s 19ms/step - loss: 0.0157 - accuracy: 0.9952 - val_loss: 0.0207 - val_accuracy: 0.9945\n",
            "Epoch 24/50\n",
            "2953/2953 [==============================] - 58s 19ms/step - loss: 0.0160 - accuracy: 0.9950 - val_loss: 0.0195 - val_accuracy: 0.9942\n",
            "Epoch 25/50\n",
            "2953/2953 [==============================] - 56s 19ms/step - loss: 0.0155 - accuracy: 0.9950 - val_loss: 0.0195 - val_accuracy: 0.9947\n",
            "Epoch 26/50\n",
            "2953/2953 [==============================] - 59s 20ms/step - loss: 0.0150 - accuracy: 0.9950 - val_loss: 0.0217 - val_accuracy: 0.9936\n",
            "Epoch 27/50\n",
            "2953/2953 [==============================] - 60s 20ms/step - loss: 0.0145 - accuracy: 0.9951 - val_loss: 0.0193 - val_accuracy: 0.9944\n",
            "Epoch 28/50\n",
            "2953/2953 [==============================] - 58s 20ms/step - loss: 0.0154 - accuracy: 0.9950 - val_loss: 0.0187 - val_accuracy: 0.9947\n",
            "Epoch 29/50\n",
            "2953/2953 [==============================] - 57s 19ms/step - loss: 0.0134 - accuracy: 0.9956 - val_loss: 0.0223 - val_accuracy: 0.9935\n",
            "Epoch 00029: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewxc1xZzU-OH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "c4a105b0-436b-430c-88a2-8c098f5cd666"
      },
      "source": [
        "# plot the training loss and accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N = 29\n",
        "H = history2\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "cnn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 125, 64)           256       \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 62, 64)            0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 70)                37800     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 70)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 355       \n",
            "=================================================================\n",
            "Total params: 38,411\n",
            "Trainable params: 38,411\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u11G0pR0ee0Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "d95b4999-92f1-4d78-d3d2-cf6a3384761f"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# make predictions on the testing set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdxs = cnn.predict(X_testc)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(y_test.argmax(axis=1), predIdxs, target_names=['DOS','Normal','Probes','R2L','U2R' ]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         DOS       1.00      1.00      1.00     11484\n",
            "      Normal       0.99      0.99      0.99     16774\n",
            "      Probes       0.99      0.98      0.98      2947\n",
            "         R2L       0.81      0.92      0.86       274\n",
            "         U2R       0.67      0.27      0.38        15\n",
            "\n",
            "    accuracy                           0.99     31494\n",
            "   macro avg       0.89      0.83      0.84     31494\n",
            "weighted avg       0.99      0.99      0.99     31494\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7x5B-CbCMqd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "efd0a171-4cff-4073-8e8e-54f28b6f0f64"
      },
      "source": [
        "pred = cnn.predict(X_testc)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "y_eval = np.argmax(y_test,axis=1)\n",
        "score = metrics.accuracy_score(y_eval, pred)\n",
        "print(\"Validation score: {}\".format(score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation score: 0.993490823648949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNcaQteXdDtw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dccdcb75-c3b2-4086-fa40-1c2118a770e2"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "lstm_output_size = 70\n",
        "num_folds = 10\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "inputs = np.concatenate((X_trainc, X_testc), axis=0)\n",
        "targets = np.concatenate((y_train, y_test), axis=0)\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    cnn = Sequential()\n",
        "    cnn.add(Convolution1D(64, 3, padding=\"same\", activation=\"relu\", input_shape=(125,1)))\n",
        "    cnn.add(MaxPooling1D(pool_size=2))\n",
        "    cnn.add(Convolution1D(128, 3, padding=\"same\", activation=\"relu\"))\n",
        "    cnn.add(MaxPooling1D(pool_size=2))\n",
        "    cnn.add(LSTM(lstm_output_size, return_sequences=True))\n",
        "    cnn.add(Dropout(0.1))\n",
        "    cnn.add(LSTM(lstm_output_size))\n",
        "    cnn.add(Dropout(0.1))\n",
        "    cnn.add(Dense(y.shape[1],activation='softmax'))\n",
        "    cnn.compile(loss='categorical_crossentropy', optimizer=\"adam\",metrics=['accuracy'])\n",
        "\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "    history2 = cnn.fit(inputs[train], targets[train], epochs=30)\n",
        "\n",
        "    scores = cnn.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {cnn.metrics_names[0]} of {scores[0]}; {cnn.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#   # Fit data to model\n",
        "#   history = model.fit(inputs[train], targets[train],\n",
        "#               batch_size=batch_size,\n",
        "#               epochs=no_epochs,\n",
        "#               verbose=verbosity)\n",
        "  # == Provide average scores ==\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.1193 - accuracy: 0.9604\n",
            "Epoch 2/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0414 - accuracy: 0.9868\n",
            "Epoch 3/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0332 - accuracy: 0.9899\n",
            "Epoch 4/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0288 - accuracy: 0.9912\n",
            "Epoch 5/30\n",
            "3543/3543 [==============================] - 23s 7ms/step - loss: 0.0248 - accuracy: 0.9925\n",
            "Epoch 6/30\n",
            "3543/3543 [==============================] - 23s 6ms/step - loss: 0.0219 - accuracy: 0.9928\n",
            "Epoch 7/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0201 - accuracy: 0.9935\n",
            "Epoch 8/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0186 - accuracy: 0.9942\n",
            "Epoch 9/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0168 - accuracy: 0.9944\n",
            "Epoch 10/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0157 - accuracy: 0.9947\n",
            "Epoch 11/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0156 - accuracy: 0.9949\n",
            "Epoch 12/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0145 - accuracy: 0.9952\n",
            "Epoch 13/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0142 - accuracy: 0.9954\n",
            "Epoch 14/30\n",
            "3543/3543 [==============================] - 23s 6ms/step - loss: 0.0131 - accuracy: 0.9956\n",
            "Epoch 15/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0125 - accuracy: 0.9957\n",
            "Epoch 16/30\n",
            "3543/3543 [==============================] - 23s 6ms/step - loss: 0.0117 - accuracy: 0.9960\n",
            "Epoch 17/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0119 - accuracy: 0.9961\n",
            "Epoch 18/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0117 - accuracy: 0.9961\n",
            "Epoch 19/30\n",
            "3543/3543 [==============================] - 23s 7ms/step - loss: 0.0116 - accuracy: 0.9960\n",
            "Epoch 20/30\n",
            "3543/3543 [==============================] - 23s 6ms/step - loss: 0.0104 - accuracy: 0.9966\n",
            "Epoch 21/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0099 - accuracy: 0.9965\n",
            "Epoch 22/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0107 - accuracy: 0.9963\n",
            "Epoch 23/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0099 - accuracy: 0.9968\n",
            "Epoch 24/30\n",
            "3543/3543 [==============================] - 23s 6ms/step - loss: 0.0097 - accuracy: 0.9964\n",
            "Epoch 25/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0087 - accuracy: 0.9969\n",
            "Epoch 26/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0096 - accuracy: 0.9966\n",
            "Epoch 27/30\n",
            "3543/3543 [==============================] - 23s 6ms/step - loss: 0.0084 - accuracy: 0.9969\n",
            "Epoch 28/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0089 - accuracy: 0.9969\n",
            "Epoch 29/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0086 - accuracy: 0.9969\n",
            "Epoch 30/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0082 - accuracy: 0.9969\n",
            "Score for fold 1: loss of 0.01467257458716631; accuracy of 99.61104989051819%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.1167 - accuracy: 0.9607\n",
            "Epoch 2/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0435 - accuracy: 0.9866\n",
            "Epoch 3/30\n",
            "3543/3543 [==============================] - 23s 6ms/step - loss: 0.0341 - accuracy: 0.9890\n",
            "Epoch 4/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0284 - accuracy: 0.9910\n",
            "Epoch 5/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0253 - accuracy: 0.9920\n",
            "Epoch 6/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0234 - accuracy: 0.9928\n",
            "Epoch 7/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0209 - accuracy: 0.9933\n",
            "Epoch 8/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0191 - accuracy: 0.9939\n",
            "Epoch 9/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0176 - accuracy: 0.9945\n",
            "Epoch 10/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0166 - accuracy: 0.9949\n",
            "Epoch 11/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0153 - accuracy: 0.9950\n",
            "Epoch 12/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0151 - accuracy: 0.9953\n",
            "Epoch 13/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0143 - accuracy: 0.9954\n",
            "Epoch 14/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0140 - accuracy: 0.9954\n",
            "Epoch 15/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0137 - accuracy: 0.9953\n",
            "Epoch 16/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0123 - accuracy: 0.9960\n",
            "Epoch 17/30\n",
            "3543/3543 [==============================] - 23s 6ms/step - loss: 0.0122 - accuracy: 0.9960\n",
            "Epoch 18/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0121 - accuracy: 0.9959\n",
            "Epoch 19/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0121 - accuracy: 0.9956\n",
            "Epoch 20/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0123 - accuracy: 0.9960\n",
            "Epoch 21/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0108 - accuracy: 0.9965\n",
            "Epoch 22/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0107 - accuracy: 0.9962\n",
            "Epoch 23/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0110 - accuracy: 0.9960\n",
            "Epoch 24/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0101 - accuracy: 0.9965\n",
            "Epoch 25/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0099 - accuracy: 0.9965\n",
            "Epoch 26/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0092 - accuracy: 0.9968\n",
            "Epoch 27/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0094 - accuracy: 0.9965\n",
            "Epoch 28/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0085 - accuracy: 0.9969\n",
            "Epoch 29/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0098 - accuracy: 0.9965\n",
            "Epoch 30/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0084 - accuracy: 0.9970\n",
            "Score for fold 2: loss of 0.00929219089448452; accuracy of 99.66661334037781%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.1219 - accuracy: 0.9602\n",
            "Epoch 2/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0449 - accuracy: 0.9859\n",
            "Epoch 3/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0369 - accuracy: 0.9883\n",
            "Epoch 4/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0314 - accuracy: 0.9897\n",
            "Epoch 5/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0272 - accuracy: 0.9913\n",
            "Epoch 6/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0236 - accuracy: 0.9924\n",
            "Epoch 7/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0219 - accuracy: 0.9928\n",
            "Epoch 8/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0196 - accuracy: 0.9938\n",
            "Epoch 9/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0189 - accuracy: 0.9936\n",
            "Epoch 10/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0173 - accuracy: 0.9944\n",
            "Epoch 11/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0159 - accuracy: 0.9945\n",
            "Epoch 12/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0152 - accuracy: 0.9951\n",
            "Epoch 13/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0141 - accuracy: 0.9954\n",
            "Epoch 14/30\n",
            "3543/3543 [==============================] - 23s 6ms/step - loss: 0.0133 - accuracy: 0.9956\n",
            "Epoch 15/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0137 - accuracy: 0.9954\n",
            "Epoch 16/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0122 - accuracy: 0.9958\n",
            "Epoch 17/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0120 - accuracy: 0.9957\n",
            "Epoch 18/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0119 - accuracy: 0.9960\n",
            "Epoch 19/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0119 - accuracy: 0.9960\n",
            "Epoch 20/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0108 - accuracy: 0.9962\n",
            "Epoch 21/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0108 - accuracy: 0.9961\n",
            "Epoch 22/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0104 - accuracy: 0.9963\n",
            "Epoch 23/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0099 - accuracy: 0.9965\n",
            "Epoch 24/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0101 - accuracy: 0.9964\n",
            "Epoch 25/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0101 - accuracy: 0.9966\n",
            "Epoch 26/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0098 - accuracy: 0.9966\n",
            "Epoch 27/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0095 - accuracy: 0.9967\n",
            "Epoch 28/30\n",
            "3543/3543 [==============================] - 23s 6ms/step - loss: 0.0098 - accuracy: 0.9965\n",
            "Epoch 29/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0094 - accuracy: 0.9966\n",
            "Epoch 30/30\n",
            "3543/3543 [==============================] - 22s 6ms/step - loss: 0.0087 - accuracy: 0.9967\n",
            "Score for fold 3: loss of 0.01093282550573349; accuracy of 99.65074062347412%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.1342 - accuracy: 0.9542\n",
            "Epoch 2/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0456 - accuracy: 0.9856\n",
            "Epoch 3/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0347 - accuracy: 0.9891\n",
            "Epoch 4/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0295 - accuracy: 0.9909\n",
            "Epoch 5/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0253 - accuracy: 0.9920\n",
            "Epoch 6/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0226 - accuracy: 0.9930\n",
            "Epoch 7/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0204 - accuracy: 0.9936\n",
            "Epoch 8/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0184 - accuracy: 0.9939\n",
            "Epoch 9/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0178 - accuracy: 0.9942\n",
            "Epoch 10/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0162 - accuracy: 0.9946\n",
            "Epoch 11/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0157 - accuracy: 0.9949\n",
            "Epoch 12/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0141 - accuracy: 0.9952\n",
            "Epoch 13/30\n",
            "3543/3543 [==============================] - 21s 6ms/step - loss: 0.0142 - accuracy: 0.9952\n",
            "Epoch 14/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0138 - accuracy: 0.9952\n",
            "Epoch 15/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0130 - accuracy: 0.9955\n",
            "Epoch 16/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0128 - accuracy: 0.9959\n",
            "Epoch 17/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0123 - accuracy: 0.9958\n",
            "Epoch 18/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0117 - accuracy: 0.9960\n",
            "Epoch 19/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0116 - accuracy: 0.9961\n",
            "Epoch 20/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0115 - accuracy: 0.9961\n",
            "Epoch 21/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0103 - accuracy: 0.9965\n",
            "Epoch 22/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0106 - accuracy: 0.9963\n",
            "Epoch 23/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0101 - accuracy: 0.9965\n",
            "Epoch 24/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0098 - accuracy: 0.9965\n",
            "Epoch 25/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0098 - accuracy: 0.9962\n",
            "Epoch 26/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0098 - accuracy: 0.9966\n",
            "Epoch 27/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0098 - accuracy: 0.9965\n",
            "Epoch 28/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0090 - accuracy: 0.9968\n",
            "Epoch 29/30\n",
            "3543/3543 [==============================] - 21s 6ms/step - loss: 0.0092 - accuracy: 0.9968\n",
            "Epoch 30/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0083 - accuracy: 0.9970\n",
            "Score for fold 4: loss of 0.012340416200459003; accuracy of 99.53957200050354%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.1393 - accuracy: 0.9561\n",
            "Epoch 2/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0440 - accuracy: 0.9865\n",
            "Epoch 3/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0351 - accuracy: 0.9893\n",
            "Epoch 4/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0290 - accuracy: 0.9912\n",
            "Epoch 5/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0253 - accuracy: 0.9919\n",
            "Epoch 6/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0217 - accuracy: 0.9927\n",
            "Epoch 7/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0209 - accuracy: 0.9931\n",
            "Epoch 8/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0188 - accuracy: 0.9939\n",
            "Epoch 9/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0182 - accuracy: 0.9940\n",
            "Epoch 10/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0160 - accuracy: 0.9945\n",
            "Epoch 11/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0154 - accuracy: 0.9951\n",
            "Epoch 12/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0138 - accuracy: 0.9954\n",
            "Epoch 13/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0135 - accuracy: 0.9952\n",
            "Epoch 14/30\n",
            "3543/3543 [==============================] - 21s 6ms/step - loss: 0.0126 - accuracy: 0.9956\n",
            "Epoch 15/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0127 - accuracy: 0.9955\n",
            "Epoch 16/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0126 - accuracy: 0.9957\n",
            "Epoch 17/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0115 - accuracy: 0.9958\n",
            "Epoch 18/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0123 - accuracy: 0.9958\n",
            "Epoch 19/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0114 - accuracy: 0.9959\n",
            "Epoch 20/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0103 - accuracy: 0.9963\n",
            "Epoch 21/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0104 - accuracy: 0.9963\n",
            "Epoch 22/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0101 - accuracy: 0.9965\n",
            "Epoch 23/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0100 - accuracy: 0.9963\n",
            "Epoch 24/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0102 - accuracy: 0.9966\n",
            "Epoch 25/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0109 - accuracy: 0.9961\n",
            "Epoch 26/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0092 - accuracy: 0.9967\n",
            "Epoch 27/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0092 - accuracy: 0.9967\n",
            "Epoch 28/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0100 - accuracy: 0.9964\n",
            "Epoch 29/30\n",
            "3543/3543 [==============================] - 21s 6ms/step - loss: 0.0093 - accuracy: 0.9967\n",
            "Epoch 30/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0082 - accuracy: 0.9971\n",
            "Score for fold 5: loss of 0.012060573324561119; accuracy of 99.51575994491577%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Epoch 1/30\n",
            "3543/3543 [==============================] - 21s 6ms/step - loss: 0.1409 - accuracy: 0.9549\n",
            "Epoch 2/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0425 - accuracy: 0.9867\n",
            "Epoch 3/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0358 - accuracy: 0.9883\n",
            "Epoch 4/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0290 - accuracy: 0.9909\n",
            "Epoch 5/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0256 - accuracy: 0.9918\n",
            "Epoch 6/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0231 - accuracy: 0.9924\n",
            "Epoch 7/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0209 - accuracy: 0.9930\n",
            "Epoch 8/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0189 - accuracy: 0.9938\n",
            "Epoch 9/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0168 - accuracy: 0.9946\n",
            "Epoch 10/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0170 - accuracy: 0.9943\n",
            "Epoch 11/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0155 - accuracy: 0.9950\n",
            "Epoch 12/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0148 - accuracy: 0.9947\n",
            "Epoch 13/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0141 - accuracy: 0.9953\n",
            "Epoch 14/30\n",
            "3543/3543 [==============================] - 21s 6ms/step - loss: 0.0144 - accuracy: 0.9950\n",
            "Epoch 15/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0130 - accuracy: 0.9954\n",
            "Epoch 16/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0126 - accuracy: 0.9956\n",
            "Epoch 17/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0124 - accuracy: 0.9958\n",
            "Epoch 18/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0118 - accuracy: 0.9960\n",
            "Epoch 19/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0119 - accuracy: 0.9961\n",
            "Epoch 20/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0108 - accuracy: 0.9962\n",
            "Epoch 21/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0106 - accuracy: 0.9963\n",
            "Epoch 22/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0108 - accuracy: 0.9962\n",
            "Epoch 23/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0109 - accuracy: 0.9961\n",
            "Epoch 24/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0107 - accuracy: 0.9964\n",
            "Epoch 25/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0095 - accuracy: 0.9965\n",
            "Epoch 26/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0104 - accuracy: 0.9964\n",
            "Epoch 27/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0093 - accuracy: 0.9966\n",
            "Epoch 28/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0103 - accuracy: 0.9963\n",
            "Epoch 29/30\n",
            "3543/3543 [==============================] - 21s 6ms/step - loss: 0.0100 - accuracy: 0.9966\n",
            "Epoch 30/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0088 - accuracy: 0.9970\n",
            "Score for fold 6: loss of 0.012139185331761837; accuracy of 99.65865015983582%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Epoch 1/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.1294 - accuracy: 0.9587\n",
            "Epoch 2/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0487 - accuracy: 0.9851\n",
            "Epoch 3/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0358 - accuracy: 0.9887\n",
            "Epoch 4/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0297 - accuracy: 0.9909\n",
            "Epoch 5/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0260 - accuracy: 0.9918\n",
            "Epoch 6/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0224 - accuracy: 0.9925\n",
            "Epoch 7/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0204 - accuracy: 0.9934\n",
            "Epoch 8/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0195 - accuracy: 0.9939\n",
            "Epoch 9/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0174 - accuracy: 0.9943\n",
            "Epoch 10/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0163 - accuracy: 0.9943\n",
            "Epoch 11/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0156 - accuracy: 0.9946\n",
            "Epoch 12/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0145 - accuracy: 0.9951\n",
            "Epoch 13/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0138 - accuracy: 0.9951\n",
            "Epoch 14/30\n",
            "3543/3543 [==============================] - 21s 6ms/step - loss: 0.0128 - accuracy: 0.9957\n",
            "Epoch 15/30\n",
            "3543/3543 [==============================] - 21s 6ms/step - loss: 0.0128 - accuracy: 0.9955\n",
            "Epoch 16/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0129 - accuracy: 0.9956\n",
            "Epoch 17/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0118 - accuracy: 0.9959\n",
            "Epoch 18/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0116 - accuracy: 0.9960\n",
            "Epoch 19/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0116 - accuracy: 0.9959\n",
            "Epoch 20/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0115 - accuracy: 0.9959\n",
            "Epoch 21/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0105 - accuracy: 0.9963\n",
            "Epoch 22/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0101 - accuracy: 0.9964\n",
            "Epoch 23/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0101 - accuracy: 0.9965\n",
            "Epoch 24/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0100 - accuracy: 0.9964\n",
            "Epoch 25/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0107 - accuracy: 0.9962\n",
            "Epoch 26/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0104 - accuracy: 0.9964\n",
            "Epoch 27/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0087 - accuracy: 0.9968\n",
            "Epoch 28/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0094 - accuracy: 0.9966\n",
            "Epoch 29/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0093 - accuracy: 0.9969\n",
            "Epoch 30/30\n",
            "3543/3543 [==============================] - 21s 6ms/step - loss: 0.0091 - accuracy: 0.9967\n",
            "Score for fold 7: loss of 0.012519162148237228; accuracy of 99.63483214378357%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Epoch 1/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.1234 - accuracy: 0.9596\n",
            "Epoch 2/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0434 - accuracy: 0.9869\n",
            "Epoch 3/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0341 - accuracy: 0.9893\n",
            "Epoch 4/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0285 - accuracy: 0.9908\n",
            "Epoch 5/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0252 - accuracy: 0.9921\n",
            "Epoch 6/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0219 - accuracy: 0.9932\n",
            "Epoch 7/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0212 - accuracy: 0.9931\n",
            "Epoch 8/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0183 - accuracy: 0.9940\n",
            "Epoch 9/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0178 - accuracy: 0.9942\n",
            "Epoch 10/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0165 - accuracy: 0.9944\n",
            "Epoch 11/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0154 - accuracy: 0.9945\n",
            "Epoch 12/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0150 - accuracy: 0.9949\n",
            "Epoch 13/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0142 - accuracy: 0.9952\n",
            "Epoch 14/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0133 - accuracy: 0.9953\n",
            "Epoch 15/30\n",
            "3543/3543 [==============================] - 21s 6ms/step - loss: 0.0123 - accuracy: 0.9957\n",
            "Epoch 16/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0120 - accuracy: 0.9956\n",
            "Epoch 17/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0120 - accuracy: 0.9957\n",
            "Epoch 18/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0118 - accuracy: 0.9959\n",
            "Epoch 19/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0115 - accuracy: 0.9959\n",
            "Epoch 20/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0108 - accuracy: 0.9961\n",
            "Epoch 21/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0113 - accuracy: 0.9960\n",
            "Epoch 22/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0105 - accuracy: 0.9962\n",
            "Epoch 23/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0103 - accuracy: 0.9963\n",
            "Epoch 24/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0098 - accuracy: 0.9965\n",
            "Epoch 25/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0098 - accuracy: 0.9964\n",
            "Epoch 26/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0092 - accuracy: 0.9967\n",
            "Epoch 27/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0094 - accuracy: 0.9967\n",
            "Epoch 28/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0093 - accuracy: 0.9967\n",
            "Epoch 29/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0094 - accuracy: 0.9966\n",
            "Epoch 30/30\n",
            "3543/3543 [==============================] - 21s 6ms/step - loss: 0.0088 - accuracy: 0.9966\n",
            "Score for fold 8: loss of 0.013575667515397072; accuracy of 99.62689280509949%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Epoch 1/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.1336 - accuracy: 0.9546\n",
            "Epoch 2/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0422 - accuracy: 0.9867\n",
            "Epoch 3/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0346 - accuracy: 0.9893\n",
            "Epoch 4/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0290 - accuracy: 0.9910\n",
            "Epoch 5/30\n",
            "3543/3543 [==============================] - 21s 6ms/step - loss: 0.0263 - accuracy: 0.9916\n",
            "Epoch 6/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0238 - accuracy: 0.9927\n",
            "Epoch 7/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0211 - accuracy: 0.9933\n",
            "Epoch 8/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0198 - accuracy: 0.9936\n",
            "Epoch 9/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0182 - accuracy: 0.9944\n",
            "Epoch 10/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0163 - accuracy: 0.9945\n",
            "Epoch 11/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0160 - accuracy: 0.9948\n",
            "Epoch 12/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0144 - accuracy: 0.9949\n",
            "Epoch 13/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0145 - accuracy: 0.9952\n",
            "Epoch 14/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0143 - accuracy: 0.9954\n",
            "Epoch 15/30\n",
            "3543/3543 [==============================] - 21s 6ms/step - loss: 0.0137 - accuracy: 0.9954\n",
            "Epoch 16/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0131 - accuracy: 0.9957\n",
            "Epoch 17/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0123 - accuracy: 0.9959\n",
            "Epoch 18/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0125 - accuracy: 0.9956\n",
            "Epoch 19/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0115 - accuracy: 0.9961\n",
            "Epoch 20/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0112 - accuracy: 0.9960\n",
            "Epoch 21/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0109 - accuracy: 0.9960\n",
            "Epoch 22/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0108 - accuracy: 0.9961\n",
            "Epoch 23/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0103 - accuracy: 0.9964\n",
            "Epoch 24/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0097 - accuracy: 0.9965\n",
            "Epoch 25/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0097 - accuracy: 0.9965\n",
            "Epoch 26/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0099 - accuracy: 0.9965\n",
            "Epoch 27/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0092 - accuracy: 0.9966\n",
            "Epoch 28/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0098 - accuracy: 0.9968\n",
            "Epoch 29/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0092 - accuracy: 0.9967\n",
            "Epoch 30/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0086 - accuracy: 0.9968\n",
            "Score for fold 9: loss of 0.017896845936775208; accuracy of 99.48400259017944%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Epoch 1/30\n",
            "3543/3543 [==============================] - 21s 6ms/step - loss: 0.1309 - accuracy: 0.9574\n",
            "Epoch 2/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0420 - accuracy: 0.9869\n",
            "Epoch 3/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0331 - accuracy: 0.9894\n",
            "Epoch 4/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0283 - accuracy: 0.9910\n",
            "Epoch 5/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0249 - accuracy: 0.9918\n",
            "Epoch 6/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0220 - accuracy: 0.9929\n",
            "Epoch 7/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0190 - accuracy: 0.9938\n",
            "Epoch 8/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0173 - accuracy: 0.9944\n",
            "Epoch 9/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0167 - accuracy: 0.9945\n",
            "Epoch 10/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0157 - accuracy: 0.9947\n",
            "Epoch 11/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0152 - accuracy: 0.9951\n",
            "Epoch 12/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0137 - accuracy: 0.9953\n",
            "Epoch 13/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0130 - accuracy: 0.9957\n",
            "Epoch 14/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0133 - accuracy: 0.9956\n",
            "Epoch 15/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0127 - accuracy: 0.9955\n",
            "Epoch 16/30\n",
            "3543/3543 [==============================] - 21s 6ms/step - loss: 0.0119 - accuracy: 0.9958\n",
            "Epoch 17/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0115 - accuracy: 0.9960\n",
            "Epoch 18/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0111 - accuracy: 0.9960\n",
            "Epoch 19/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0111 - accuracy: 0.9965\n",
            "Epoch 20/30\n",
            "3543/3543 [==============================] - 21s 6ms/step - loss: 0.0110 - accuracy: 0.9961\n",
            "Epoch 21/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0106 - accuracy: 0.9962\n",
            "Epoch 22/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0106 - accuracy: 0.9962\n",
            "Epoch 23/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0102 - accuracy: 0.9962\n",
            "Epoch 24/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0104 - accuracy: 0.9965\n",
            "Epoch 25/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0094 - accuracy: 0.9967\n",
            "Epoch 26/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0099 - accuracy: 0.9964\n",
            "Epoch 27/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0098 - accuracy: 0.9965\n",
            "Epoch 28/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0092 - accuracy: 0.9967\n",
            "Epoch 29/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0091 - accuracy: 0.9969\n",
            "Epoch 30/30\n",
            "3543/3543 [==============================] - 20s 6ms/step - loss: 0.0085 - accuracy: 0.9969\n",
            "Score for fold 10: loss of 0.011763829737901688; accuracy of 99.6110200881958%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XgLH-aqdw2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "b066a401-3729-4736-9688-3d37602a10ce"
      },
      "source": [
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.01467257458716631 - Accuracy: 99.61104989051819%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.00929219089448452 - Accuracy: 99.66661334037781%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.01093282550573349 - Accuracy: 99.65074062347412%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.012340416200459003 - Accuracy: 99.53957200050354%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.012060573324561119 - Accuracy: 99.51575994491577%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.012139185331761837 - Accuracy: 99.65865015983582%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.012519162148237228 - Accuracy: 99.63483214378357%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.013575667515397072 - Accuracy: 99.62689280509949%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.017896845936775208 - Accuracy: 99.48400259017944%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.011763829737901688 - Accuracy: 99.6110200881958%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 99.59991335868835 (+- 0.060691916173598995)\n",
            "> Loss: 0.012719327118247747\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMwWhxyg-Zej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "943a7c21-f2b9-45f8-e5c9-ac7cbcf65321"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "# make predictions on the testing set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdxs = cnn.predict(X_testc)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "print(predIdxs[:2])\n",
        "print(y_test[:2])\n",
        "temp = np.array([list(x).index(1) for x in y_test])\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(y_test.argmax(axis=1), predIdxs, target_names=['DOS','Normal','Probes','R2L','U2R' ]))\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(temp, predIdxs, [0,1,2,3,4 ]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating network...\n",
            "[1 0]\n",
            "[[0 1 0 0 0]\n",
            " [1 0 0 0 0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         DOS       1.00      1.00      1.00     11484\n",
            "      Normal       1.00      1.00      1.00     16774\n",
            "      Probes       1.00      0.99      0.99      2947\n",
            "         R2L       0.93      0.88      0.91       274\n",
            "         U2R       0.88      0.47      0.61        15\n",
            "\n",
            "    accuracy                           1.00     31494\n",
            "   macro avg       0.96      0.87      0.90     31494\n",
            "weighted avg       1.00      1.00      1.00     31494\n",
            "\n",
            "Confusion Matrix\n",
            "[[11472    12     0     0     0]\n",
            " [    5 16738    13    17     1]\n",
            " [    0    19  2928     0     0]\n",
            " [    0    32     0   242     0]\n",
            " [    0     7     1     0     7]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}